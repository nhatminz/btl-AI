{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":7020161,"sourceId":11237152,"sourceType":"datasetVersion"}],"dockerImageVersionId":31011,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"papermill":{"default_parameters":{},"duration":9723.733886,"end_time":"2025-05-01T16:08:43.880075","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-01T13:26:40.146189","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a2a68fe9","cell_type":"code","source":"!pip install numpy torch\n!pip install GDAL==$(gdal-config --version)\n!pip install scikit-learn matplotlib cartopy shapely","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:26:45.165329Z","iopub.status.busy":"2025-05-01T13:26:45.164989Z","iopub.status.idle":"2025-05-01T13:28:32.278907Z","shell.execute_reply":"2025-05-01T13:28:32.277397Z"},"papermill":{"duration":107.122959,"end_time":"2025-05-01T13:28:32.282532","exception":false,"start_time":"2025-05-01T13:26:45.159573","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\r\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\r\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\r\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\r\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n","  Attempting uninstall: nvidia-nvjitlink-cu12\r\n","    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n","    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n","  Attempting uninstall: nvidia-curand-cu12\r\n","    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n","    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n","      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n","  Attempting uninstall: nvidia-cufft-cu12\r\n","    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n","    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n","  Attempting uninstall: nvidia-cublas-cu12\r\n","    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n","    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n","      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n","  Attempting uninstall: nvidia-cusparse-cu12\r\n","    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n","    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n","  Attempting uninstall: nvidia-cudnn-cu12\r\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n","  Attempting uninstall: nvidia-cusolver-cu12\r\n","    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n","    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n","pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n","Requirement already satisfied: GDAL==3.6.4 in /usr/local/lib/python3.11/dist-packages (3.6.4)\r\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\r\n","Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\r\n","Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.0)\r\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\r\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\r\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\r\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\r\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\r\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\r\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\r\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\r\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\r\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\r\n","Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\r\n","Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.0)\r\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\r\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\r\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\r\n","Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\r\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\r\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\r\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.1.31)\r\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n","Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\r\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n"]}],"execution_count":1},{"id":"56d5400f","cell_type":"code","source":"!ls -l /kaggle/input/btl-ai/DATA_SV/Hima/B04B","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:32.345852Z","iopub.status.busy":"2025-05-01T13:28:32.345448Z","iopub.status.idle":"2025-05-01T13:28:32.482085Z","shell.execute_reply":"2025-05-01T13:28:32.481017Z"},"papermill":{"duration":0.171204,"end_time":"2025-05-01T13:28:32.483958","exception":false,"start_time":"2025-05-01T13:28:32.312754","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["total 0\r\n","drwxr-xr-x 4 nobody nogroup 0 Apr 27 06:37 2019\r\n","drwxr-xr-x 4 nobody nogroup 0 Apr 27 06:37 2020\r\n"]}],"execution_count":2},{"id":"09d5bbea","cell_type":"code","source":"import os\nimport numpy as np\nfrom osgeo import gdal\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport logging\n\n# Thiết lập logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Thiết lập thiết bị\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Đặt seed để tái lập kết quả\ntorch.manual_seed(42)\nnp.random.seed(42)","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:32.543514Z","iopub.status.busy":"2025-05-01T13:28:32.543115Z","iopub.status.idle":"2025-05-01T13:28:40.201353Z","shell.execute_reply":"2025-05-01T13:28:40.200154Z"},"papermill":{"duration":7.689682,"end_time":"2025-05-01T13:28:40.203043","exception":false,"start_time":"2025-05-01T13:28:32.513361","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]}],"execution_count":3},{"id":"0cf3c01b","cell_type":"code","source":"# Hằng số và đường dẫn\nBASE_PATH = \"/kaggle/input/btl-ai/DATA_SV\"\nHIMA_PATH = os.path.join(BASE_PATH, \"Hima\")\nERA5_PATH = os.path.join(BASE_PATH, \"ERA5\")\nPRECIP_PATH = os.path.join(BASE_PATH, \"Precipitation/Radar\")\nOUTPUT_PATH = \"/kaggle/working/output/\"\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n\nHIMA_BANDS = ['B04B', 'B05B', 'B06B', 'B09B', 'B10B', 'B11B', 'B12B', 'B14B', 'B16B', 'I2B', 'I4B', 'IRB', 'VSB', 'WVB']  # 14 band\nERA5_PARAMS = ['CAPE', 'CIN', 'EWSS', 'IE', 'ISOR', 'KX', 'PEV', 'R250', 'R500', 'R850', 'SLHF', 'SLOR', 'SSHF', 'TCLW', 'TCW', 'TCWV', 'U250', 'U850', 'V250', 'V850']  # 20 tham số\nHEIGHT, WIDTH = 90, 250\nNUM_EPOCHS = 20\nBATCH_SIZE = 2\n\n# Hàm đọc file GeoTIFF\ndef read_geotiff(file_path):\n    try:\n        ds = gdal.Open(file_path)\n        band = ds.GetRasterBand(1)\n        data = band.ReadAsArray().astype(np.float32)\n        ds = None\n        if data.shape != (HEIGHT, WIDTH):\n            logging.warning(f\"Invalid shape {data.shape} for file {file_path}, expected ({HEIGHT}, {WIDTH})\")\n            return None\n        return data\n    except Exception as e:\n        logging.error(f\"Error reading {file_path}: {e}\")\n        return None","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.261661Z","iopub.status.busy":"2025-05-01T13:28:40.261114Z","iopub.status.idle":"2025-05-01T13:28:40.269973Z","shell.execute_reply":"2025-05-01T13:28:40.268912Z"},"papermill":{"duration":0.040155,"end_time":"2025-05-01T13:28:40.271754","exception":false,"start_time":"2025-05-01T13:28:40.231599","status":"completed"},"tags":[]},"outputs":[],"execution_count":4},{"id":"e71375d5","cell_type":"code","source":"# Hàm phân tích thời gian từ tên file\ndef parse_datetime_from_filename(filename, data_type):\n    try:\n        if data_type == \"Hima\":\n            time_part = filename.split('_')[1].split('_TB.tif')[0].replace('.Z', '')\n            dt = datetime.strptime(time_part, '%Y%m%d%H%M')\n        elif data_type in [\"ERA5\", \"Radar\"]:\n            time_part = filename.split('_')[1].replace('.tif', '')\n            dt = datetime.strptime(time_part, '%Y%m%d%H%M%S')\n        else:\n            return None\n        return dt.replace(minute=0, second=0, microsecond=0)\n    except Exception as e:\n        logging.warning(f\"Error parsing datetime from {filename} (type {data_type}): {e}\")\n        return None\n\n# Hàm thu thập file\ndef collect_files(base_path, expected_subdirs=None, data_type=None, current_band=None):\n    files_dict = {}\n    file_count = 0\n    for root, _, files in os.walk(base_path):\n        for file in files:\n            if file.endswith('.tif'):\n                file_path = os.path.join(root, file)\n                dt = parse_datetime_from_filename(file, data_type)\n                if dt is None:\n                    continue\n                file_count += 1\n                if expected_subdirs:\n                    if current_band is None:\n                        logging.warning(f\"current_band not provided, skipping {file_path}\")\n                        continue\n                    if dt not in files_dict:\n                        files_dict[dt] = {}\n                    files_dict[dt][current_band] = file_path\n                else:\n                    files_dict[dt] = file_path\n    logging.info(f\"Found {file_count} files in {base_path}\")\n    return files_dict\n\n# Hàm tính trung bình toàn cục\ndef compute_global_means(files_dict, bands, data_type):\n    logging.info(f\"Computing global means for {data_type}...\")\n    global_means = {band: [] for band in bands}\n    for dt, paths in files_dict.items():\n        for band in bands:\n            file_path = paths.get(band)\n            if file_path and os.path.exists(file_path):\n                data = read_geotiff(file_path)\n                if data is not None:\n                    valid_data = data[~(np.isinf(data) | np.isnan(data) | (data == -9999))]\n                    if valid_data.size > 0:\n                        global_means[band].append(np.mean(valid_data))\n    \n    for band in bands:\n        if global_means[band]:\n            global_means[band] = np.mean(global_means[band])\n        else:\n            global_means[band] = 0\n            logging.warning(f\"No valid data for {data_type} band {band}, using 0 as global mean\")\n    \n    logging.info(f\"Completed global means for {data_type}\")\n    return global_means","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.455841Z","iopub.status.busy":"2025-05-01T13:28:40.455518Z","iopub.status.idle":"2025-05-01T13:28:40.470045Z","shell.execute_reply":"2025-05-01T13:28:40.468980Z"},"papermill":{"duration":0.171201,"end_time":"2025-05-01T13:28:40.471962","exception":false,"start_time":"2025-05-01T13:28:40.300761","status":"completed"},"tags":[]},"outputs":[],"execution_count":5},{"id":"3b980c06","cell_type":"code","source":"# Hàm xử lý dữ liệu thiếu và chuẩn hóa\ndef preprocess_data(data, data_type, missing_threshold=0.7):\n    if data is None:\n        logging.warning(\"Received None data for preprocessing\")\n        return None, None\n    \n    invalid_mask = np.isinf(data) | np.isnan(data) | (data == -9999)\n    invalid_ratio = np.sum(invalid_mask) / data.size\n    if invalid_ratio > missing_threshold:\n        logging.warning(f\"Data has {invalid_ratio*100:.2f}% invalid values, skipping\")\n        return None, None\n    \n    if data_type == \"Radar\":\n        data = np.log1p(np.maximum(data, 0))\n    else:\n        data_min, data_max = np.nanmin(data), np.nanmax(data)\n        if data_max > data_min:\n            data = (data - data_min) / (data_max - data_min)\n        else:\n            data = np.zeros_like(data)\n    \n    return data, invalid_mask\n\n# Hàm điền giá trị thiếu\ndef fill_missing_values(sequence, invalid_masks, sequence_dts, global_means, data_type=\"Hima\"):\n    sequence = sequence.copy()\n    n_frames, height, width, n_channels = sequence.shape\n    bands = HIMA_BANDS if data_type == \"Hima\" else ERA5_PARAMS\n    \n    for c in range(n_channels):\n        band_idx = c if data_type == \"Hima\" else c - len(HIMA_BANDS)\n        if band_idx >= len(bands) or band_idx < 0:\n            continue\n        band = bands[band_idx]\n        \n        for t in range(n_frames):\n            mask = invalid_masks[t, :, :, c]\n            if not np.any(mask):\n                continue\n\n            # Step 1: Forward fill (≤ 2 hours)\n            for t_prev in range(t-1, -1, -1):\n                time_diff = (sequence_dts[t] - sequence_dts[t_prev]).total_seconds() / 3600\n                if time_diff <= 2 and not np.any(invalid_masks[t_prev, :, :, c][mask]):\n                    sequence[t, :, :, c][mask] = sequence[t_prev, :, :, c][mask]\n                    invalid_masks[t, :, :, c][mask] = False\n                    logging.info(f\"Forward filled channel {c}, frame {t}\")\n                    break\n\n            # Step 2: Backward fill (≤ 2 hours)\n            if np.any(mask):\n                for t_next in range(t+1, n_frames):\n                    time_diff = (sequence_dts[t_next] - sequence_dts[t]).total_seconds() / 3600\n                    if time_diff <= 2 and not np.any(invalid_masks[t_next, :, :, c][mask]):\n                        sequence[t, :, :, c][mask] = sequence[t_next, :, :, c][mask]\n                        invalid_masks[t, :, :, c][mask] = False\n                        logging.info(f\"Backward filled channel {c}, frame {t}\")\n                        break\n\n            # Step 3: Linear Interpolation (≤ 4 hours)\n            if np.any(mask):\n                t_prev, t_next = None, None\n                for t_p in range(t-1, -1, -1):\n                    time_diff = (sequence_dts[t] - sequence_dts[t_p]).total_seconds() / 3600\n                    if time_diff <= 4 and not np.any(invalid_masks[t_p, :, :, c][mask]):\n                        t_prev = t_p\n                        break\n                for t_n in range(t+1, n_frames):\n                    time_diff = (sequence_dts[t_n] - sequence_dts[t]).total_seconds() / 3600\n                    if time_diff <= 4 and not np.any(invalid_masks[t_n, :, :, c][mask]):\n                        t_next = t_n\n                        break\n                if t_prev is not None and t_next is not None:\n                    time_prev = (sequence_dts[t] - sequence_dts[t_prev]).total_seconds() / 3600\n                    time_next = (sequence_dts[t_next] - sequence_dts[t]).total_seconds() / 3600\n                    total_time = time_prev + time_next\n                    if total_time > 0:\n                        weight_prev = time_next / total_time\n                        weight_next = time_prev / total_time\n                        sequence[t, :, :, c][mask] = (\n                            weight_prev * sequence[t_prev, :, :, c][mask] +\n                            weight_next * sequence[t_next, :, :, c][mask]\n                        )\n                        invalid_masks[t, :, :, c][mask] = False\n                        logging.info(f\"Linearly interpolated channel {c}, frame {t}\")\n\n            # Step 4: Fallback - Global mean\n            if np.any(mask):\n                sequence[t, :, :, c][mask] = global_means[band]\n                logging.info(f\"Filled channel {c}, frame {t} with global mean {global_means[band]}\")\n\n    return sequence","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.537024Z","iopub.status.busy":"2025-05-01T13:28:40.535969Z","iopub.status.idle":"2025-05-01T13:28:40.563481Z","shell.execute_reply":"2025-05-01T13:28:40.562491Z"},"papermill":{"duration":0.063486,"end_time":"2025-05-01T13:28:40.565373","exception":false,"start_time":"2025-05-01T13:28:40.501887","status":"completed"},"tags":[]},"outputs":[],"execution_count":6},{"id":"6ad9a7e7","cell_type":"code","source":"# Hàm tạo chuỗi thời gian\ndef create_time_sequences(hima_files, era5_files, precip_files, common_datetimes, hima_global_means, era5_global_means):\n    logging.info(\"Starting to create time sequences...\")\n    def generate_sequences():\n        for i in range(4, len(common_datetimes)):\n            dt = common_datetimes[i]\n            valid_sequence = True\n            for j in range(1, 5):\n                if common_datetimes[i-j] != dt - timedelta(hours=j):\n                    valid_sequence = False\n                    break\n            if not valid_sequence:\n                continue\n\n            sequence = []\n            invalid_masks = []\n            sequence_dts = []\n            for j in range(5):\n                dt_j = common_datetimes[i-j]\n                if dt_j not in hima_files or dt_j not in era5_files:\n                    valid_sequence = False\n                    break\n\n                # Đọc Himawari\n                hima_data = []\n                hima_masks = []\n                for band in HIMA_BANDS:\n                    file_path = hima_files[dt_j].get(band)\n                    if not file_path or not os.path.exists(file_path):\n                        valid_sequence = False\n                        break\n                    data = read_geotiff(file_path)\n                    if data is None:\n                        valid_sequence = False\n                        break\n                    data, mask = preprocess_data(data, \"Hima\")\n                    if data is None:\n                        valid_sequence = False\n                        break\n                    hima_data.append(data)\n                    hima_masks.append(mask)\n                if not valid_sequence:\n                    break\n                hima_data = np.stack(hima_data, axis=-1)\n                hima_masks = np.stack(hima_masks, axis=-1)\n\n                # Đọc ERA5\n                era5_data = []\n                era5_masks = []\n                for param in ERA5_PARAMS:\n                    file_path = era5_files[dt_j].get(param)\n                    if not file_path or not os.path.exists(file_path):\n                        valid_sequence = False\n                        break\n                    data = read_geotiff(file_path)\n                    if data is None:\n                        valid_sequence = False\n                        break\n                    data, mask = preprocess_data(data, \"ERA5\")\n                    if data is None:\n                        valid_sequence = False\n                        break\n                    era5_data.append(data)\n                    era5_masks.append(mask)\n                if not valid_sequence:\n                    break\n                era5_data = np.stack(era5_data, axis=-1)\n                era5_masks = np.stack(era5_masks, axis=-1)\n\n                combined = np.concatenate([hima_data, era5_data], axis=-1)\n                combined_masks = np.concatenate([hima_masks, era5_masks], axis=-1)\n                sequence.append(combined)\n                invalid_masks.append(combined_masks)\n                sequence_dts.append(dt_j)\n            if not valid_sequence:\n                continue\n\n            # Đọc radar\n            radar_file = precip_files.get(dt)\n            if not radar_file or not os.path.exists(radar_file):\n                continue\n            radar_data = read_geotiff(radar_file)\n            if radar_data is None:\n                continue\n            radar_data, _ = preprocess_data(radar_data, \"Radar\")\n            if radar_data is None:\n                continue\n\n            # Điền giá trị thiếu\n            sequence = np.stack(sequence, axis=0)\n            invalid_masks = np.stack(invalid_masks, axis=0)\n            sequence = fill_missing_values(sequence, invalid_masks, sequence_dts, hima_global_means, \"Hima\")\n            sequence = fill_missing_values(sequence, invalid_masks, sequence_dts, era5_global_means, \"ERA5\")\n\n            sequence = sequence.transpose(0, 3, 1, 2)\n            yield sequence, radar_data\n\n    X, y = [], []\n    for seq, target in generate_sequences():\n        X.append(seq)\n        y.append(target)\n    \n    logging.info(f\"Completed time sequences, generated {len(X)} samples\")\n    return np.array(X), np.array(y)","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.636700Z","iopub.status.busy":"2025-05-01T13:28:40.636309Z","iopub.status.idle":"2025-05-01T13:28:40.655378Z","shell.execute_reply":"2025-05-01T13:28:40.654399Z"},"papermill":{"duration":0.053711,"end_time":"2025-05-01T13:28:40.657530","exception":false,"start_time":"2025-05-01T13:28:40.603819","status":"completed"},"tags":[]},"outputs":[],"execution_count":7},{"id":"5d7cdaca","cell_type":"code","source":"# Định nghĩa lớp ConvLSTMCell\nclass ConvLSTMCell(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding):\n        super(ConvLSTMCell, self).__init__()\n        self.out_channels = out_channels\n        self.conv = nn.Conv2d(\n            in_channels + out_channels, 4 * out_channels, kernel_size,\n            padding=padding, bias=True\n        )\n\n    def forward(self, x, h_prev, c_prev):\n        combined = torch.cat([x, h_prev], dim=1)\n        conv_out = self.conv(combined)\n        i, f, o, g = torch.chunk(conv_out, 4, dim=1)\n        i = torch.sigmoid(i)\n        f = torch.sigmoid(f)\n        o = torch.sigmoid(o)\n        g = torch.tanh(g)\n        c_next = f * c_prev + i * g\n        h_next = o * torch.tanh(c_next)\n        return h_next, c_next\n\n# Định nghĩa lớp ConvLSTM2d\nclass ConvLSTM2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding):\n        super(ConvLSTM2d, self).__init__()\n        self.cell = ConvLSTMCell(in_channels, out_channels, kernel_size, padding)\n\n    def forward(self, x):\n        if len(x.size()) == 5:\n            batch, seq_len, channels, height, width = x.size()\n            is_sequence = True\n        elif len(x.size()) == 4:\n            batch, channels, height, width = x.size()\n            seq_len = 1\n            x = x.unsqueeze(1)\n            is_sequence = False\n        else:\n            raise ValueError(f\"Expected 4 or 5 dimensions, got {len(x.size())}\")\n\n        out_channels = self.cell.out_channels\n        h = torch.zeros(batch, out_channels, height, width, device=x.device)\n        c = torch.zeros(batch, out_channels, height, width, device=x.device)\n        outputs = []\n        for t in range(seq_len):\n            x_t = x[:, t, :, :, :]\n            h, c = self.cell(x_t, h, c)\n            outputs.append(h)\n        output = outputs[-1] if is_sequence else h\n        return output, (h, c)\n\n# Định nghĩa mô hình ConvLSTM\nclass ConvLSTMModel(nn.Module):\n    def __init__(self):\n        super(ConvLSTMModel, self).__init__()\n        self.convlstm1 = ConvLSTM2d(in_channels=34, out_channels=64, kernel_size=(5, 5), padding=(2, 2))\n        self.bn1 = nn.BatchNorm2d(64)\n        self.convlstm2 = ConvLSTM2d(in_channels=64, out_channels=32, kernel_size=(5, 5), padding=(2, 2))\n        self.bn2 = nn.BatchNorm2d(32)\n        self.dropout = nn.Dropout(0.2)\n        self.conv = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=(3, 3), padding=(1, 1))\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x, _ = self.convlstm1(x)\n        x = self.bn1(x)\n        x, _ = self.convlstm2(x)\n        x = self.bn2(x)\n        x = self.dropout(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        return x.squeeze(1)","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.727085Z","iopub.status.busy":"2025-05-01T13:28:40.726726Z","iopub.status.idle":"2025-05-01T13:28:40.744319Z","shell.execute_reply":"2025-05-01T13:28:40.743278Z"},"papermill":{"duration":0.05099,"end_time":"2025-05-01T13:28:40.746148","exception":false,"start_time":"2025-05-01T13:28:40.695158","status":"completed"},"tags":[]},"outputs":[],"execution_count":8},{"id":"2a6d660c","cell_type":"code","source":"# Hàm huấn luyện mô hình\ndef train_model(model, train_loader, val_loader, epochs=NUM_EPOCHS, patience=5):\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n    best_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            output = model(X_batch)\n            loss = criterion(output, y_batch)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * X_batch.size(0)\n        train_loss /= len(train_loader.dataset)\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                output = model(X_batch)\n                loss = criterion(output, y_batch)\n                val_loss += loss.item() * X_batch.size(0)\n            val_loss /= len(val_loader.dataset)\n\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n\n        if val_loss < best_loss:\n            best_loss = val_loss\n            best_model_state = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping\")\n                break\n\n    model.load_state_dict(best_model_state)\n    return model\n\n# Hàm tính chỉ số đánh giá\ndef evaluate_model(y_true, y_pred, threshold=0.0):\n    y_true = y_true.reshape(-1)\n    y_pred = y_pred.reshape(-1)\n    valid_mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n    y_true = y_true[valid_mask]\n    y_pred = y_pred[valid_mask]\n    \n    rmse = np.sqrt(mean_squared_error(y_true, y_pred)) if len(y_true) > 0 else float('inf')\n    corr = np.corrcoef(y_true, y_pred)[0, 1] if len(y_true) > 1 and np.std(y_true) > 0 and np.std(y_pred) > 0 else 0\n\n    y_true_bin = (y_true > threshold).astype(int)\n    y_pred_bin = (y_pred > threshold).astype(int)\n    hits = np.sum((y_true_bin == 1) & (y_pred_bin == 1))\n    misses = np.sum((y_true_bin == 1) & (y_pred_bin == 0))\n    false_alarms = np.sum((y_true_bin == 0) & (y_pred_bin == 1))\n    true_negatives = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n    total = hits + misses + false_alarms + true_negatives\n\n    accuracy = (hits + true_negatives) / total if total > 0 else 0\n    csi = hits / (hits + misses + false_alarms) if (hits + misses + false_alarms) > 0 else 0\n    far = false_alarms / (hits + false_alarms) if (hits + false_alarms) > 0 else 0\n    hss = (2 * (hits * true_negatives - misses * false_alarms)) / \\\n          ((hits + misses) * (misses + true_negatives) + (hits + false_alarms) * (false_alarms + true_negatives)) \\\n          if ((hits + misses) * (misses + true_negatives) + (hits + false_alarms) * (false_alarms + true_negatives)) > 0 else 0\n    ets = ((hits - ((hits + misses) * (hits + false_alarms) / total)) / \\\n           (hits + misses + false_alarms - ((hits + misses) * (hits + false_alarms) / total))) \\\n          if (hits + misses + false_alarms - ((hits + misses) * (hits + false_alarms) / total)) > 0 else 0\n\n    return {'rmse': rmse, 'corr': corr, 'accuracy': accuracy, 'csi': csi, 'far': far, 'hss': hss, 'ets': ets}\n\n# Hàm vẽ scatter plot\ndef plot_scatter(y_true, y_pred, output_path):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_true.flatten(), y_pred.flatten(), alpha=0.5)\n    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n    plt.xlabel('Ground Truth (mm/h)')\n    plt.ylabel('Predicted (mm/h)')\n    plt.title('Scatter Plot: Predicted vs Ground Truth')\n    plt.savefig(output_path)\n    plt.close()\n\n# Hàm hiển thị bản đồ\ndef plot_rainfall_map(y_true, y_pred, output_path):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), subplot_kw={'projection': ccrs.PlateCarree()})\n    ax1.set_title('Ground Truth')\n    ax2.set_title('Prediction')\n    for ax, data in [(ax1, y_true), (ax2, y_pred)]:\n        ax.coastlines()\n        ax.add_feature(cfeature.BORDERS)\n        im = ax.imshow(data, cmap='Blues', origin='upper', transform=ccrs.PlateCarree())\n        plt.colorbar(im, ax=ax, label='Rainfall (mm/h)')\n    plt.savefig(output_path)\n    plt.close()\n\n# Hàm lưu GeoTIFF\ndef save_geotiff(data, output_path, reference_file):\n    ds = gdal.Open(reference_file)\n    driver = gdal.GetDriverByName('GTiff')\n    out_ds = driver.Create(output_path, WIDTH, HEIGHT, 1, gdal.GDT_Float32)\n    out_ds.SetGeoTransform(ds.GetGeoTransform())\n    out_ds.SetProjection(ds.GetProjection())\n    out_band = out_ds.GetRasterBand(1)\n    out_band.WriteArray(data)\n    out_band.FlushCache()\n    out_ds = None\n    ds = None","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.808347Z","iopub.status.busy":"2025-05-01T13:28:40.807984Z","iopub.status.idle":"2025-05-01T13:28:40.827941Z","shell.execute_reply":"2025-05-01T13:28:40.826979Z"},"papermill":{"duration":0.05107,"end_time":"2025-05-01T13:28:40.829468","exception":false,"start_time":"2025-05-01T13:28:40.778398","status":"completed"},"tags":[]},"outputs":[],"execution_count":9},{"id":"f4c2f4c8","cell_type":"code","source":"# Phần chính\n# Thu thập file\nlogging.info(\"Collecting Himawari files...\")\nhima_files_raw = {}\nfor band in HIMA_BANDS:\n    band_path = os.path.join(HIMA_PATH, band)\n    if not os.path.exists(band_path):\n        logging.warning(f\"Directory not found: {band_path}\")\n        continue\n    band_files = collect_files(band_path, expected_subdirs=HIMA_BANDS, data_type=\"Hima\", current_band=band)\n    for dt, paths in band_files.items():\n        if dt not in hima_files_raw:\n            hima_files_raw[dt] = {}\n        hima_files_raw[dt][band] = paths[band]\n\n# Lọc các thời điểm có đủ tất cả band\nhima_files = {}\nfor dt, bands in hima_files_raw.items():\n    if all(band in bands for band in HIMA_BANDS):\n        hima_files[dt] = bands\n    else:\n        logging.warning(f\"Datetime {dt} is missing some Himawari bands, skipping\")\n\nlogging.info(\"Collecting ERA5 files...\")\nera5_files_raw = {}\nfor param in ERA5_PARAMS:\n    param_path = os.path.join(ERA5_PATH, param)\n    if not os.path.exists(param_path):\n        logging.warning(f\"Directory not found: {param_path}\")\n        continue\n    param_files = collect_files(param_path, expected_subdirs=ERA5_PARAMS, data_type=\"ERA5\", current_band=param)\n    for dt, paths in param_files.items():\n        if dt not in era5_files_raw:\n            era5_files_raw[dt] = {}\n        era5_files_raw[dt][param] = paths[param]\n\n# Lọc các thời điểm có đủ tất cả param\nera5_files = {}\nfor dt, params in era5_files_raw.items():\n    if all(param in params for param in ERA5_PARAMS):\n        era5_files[dt] = params\n    else:\n        logging.warning(f\"Datetime {dt} is missing some ERA5 params, skipping\")\n\nlogging.info(\"Collecting Precipitation files...\")\nprecip_files = collect_files(PRECIP_PATH, data_type=\"Radar\")\n\n# Tìm thời gian chung\ncommon_datetimes = sorted(list(set(hima_files.keys()) & set(era5_files.keys()) & set(precip_files.keys())))\nlogging.info(f\"Found {len(common_datetimes)} common datetimes\")\n\n# Tính global means\nhima_global_means = compute_global_means(hima_files, HIMA_BANDS, \"Hima\")\nera5_global_means = compute_global_means(era5_files, ERA5_PARAMS, \"ERA5\")\n\n# Tạo chuỗi thời gian\nX, y = create_time_sequences(hima_files, era5_files, precip_files, common_datetimes, hima_global_means, era5_global_means)\n\n# Chia dữ liệu\nlogging.info(\"Splitting data into train/val/test...\")\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\nlogging.info(f\"Train: {len(X_train)} samples, Val: {len(X_val)} samples, Test: {len(X_test)} samples\")\n\n# Chuyển sang tensor\nX_train = torch.tensor(X_train, dtype=torch.float32).to(device)\ny_train = torch.tensor(y_train, dtype=torch.float32).to(device)\nX_val = torch.tensor(X_val, dtype=torch.float32).to(device)\ny_val = torch.tensor(y_val, dtype=torch.float32).to(device)\nX_test = torch.tensor(X_test, dtype=torch.float32).to(device)\ny_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n\n# Tạo DataLoader\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\ntest_dataset = TensorDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n\n# Khởi tạo mô hình\nmodel = ConvLSTMModel().to(device)\nlogging.info(\"Initialized ConvLSTM model\")\n\n# Xóa cache GPU\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    logging.info(\"Cleared GPU cache\")\n\n# Huấn luyện mô hình\nlogging.info(\"Starting training...\")\nmodel = train_model(model, train_loader, val_loader)\n\n# Lưu mô hình\ntorch.save(model.state_dict(), os.path.join(OUTPUT_PATH, \"convlstm_model.pth\"))\nlogging.info(\"Model saved to convlstm_model.pth\")\n\n# Đánh giá trên tập kiểm thử\nlogging.info(\"Evaluating on test set...\")\nmodel.eval()\ny_pred = []\nwith torch.no_grad():\n    for X_batch, _ in test_loader:\n        X_batch = X_batch.to(device)\n        output = model(X_batch)\n        y_pred.append(output.cpu().numpy())\ny_pred = np.concatenate(y_pred, axis=0)\n\n# Đánh giá\nmetrics = evaluate_model(y_test.numpy(), y_pred)\nprint(\"Evaluation Metrics:\", metrics)\n\n# Vẽ scatter plot\nplot_scatter(y_test.numpy(), y_pred, os.path.join(OUTPUT_PATH, 'scatter_plot.png'))\n\n# Vẽ bản đồ cho mẫu đầu tiên\nplot_rainfall_map(y_test[0].numpy(), y_pred[0], os.path.join(OUTPUT_PATH, 'rainfall_map.png'))\n\n# Lưu bản đồ dự đoán dưới dạng GeoTIFF\nsave_geotiff(y_pred[0], os.path.join(OUTPUT_PATH, 'predicted_rainfall.tif'),\n             precip_files[common_datetimes[-1]])","metadata":{"execution":{"iopub.execute_input":"2025-05-01T13:28:40.887925Z","iopub.status.busy":"2025-05-01T13:28:40.887607Z","iopub.status.idle":"2025-05-01T16:08:41.213393Z","shell.execute_reply":"2025-05-01T16:08:41.211738Z"},"papermill":{"duration":9600.358171,"end_time":"2025-05-01T16:08:41.216160","exception":false,"start_time":"2025-05-01T13:28:40.857989","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20, Train Loss: 0.1116, Val Loss: 0.0673\n","Epoch 2/20, Train Loss: 0.0777, Val Loss: 0.0663\n","Epoch 3/20, Train Loss: 0.0653, Val Loss: 0.0658\n","Epoch 4/20, Train Loss: 0.0692, Val Loss: 0.0617\n","Epoch 5/20, Train Loss: 0.0592, Val Loss: 0.0698\n","Epoch 6/20, Train Loss: 0.0582, Val Loss: 0.0635\n","Epoch 7/20, Train Loss: 0.0551, Val Loss: 0.0673\n","Epoch 8/20, Train Loss: 0.0540, Val Loss: 0.0621\n","Epoch 9/20, Train Loss: 0.0508, Val Loss: 0.0676\n","Early stopping\n","Evaluation Metrics: {'rmse': 0.19621901, 'corr': 0.6616477218956265, 'accuracy': 0.8937179138321996, 'csi': 0.2568134105424724, 'far': 0.6998443291326909, 'hss': 0.3585643622798681, 'ets': 0.2184455814410702}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/110m_physical/ne_110m_coastline.zip\n","  warnings.warn(f'Downloading: {url}', DownloadWarning)\n","/usr/local/lib/python3.11/dist-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_boundary_lines_land.zip\n","  warnings.warn(f'Downloading: {url}', DownloadWarning)\n"]}],"execution_count":10}]}