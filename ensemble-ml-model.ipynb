{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11237152,"sourceType":"datasetVersion","datasetId":7020161},{"sourceId":11651856,"sourceType":"datasetVersion","datasetId":7310237}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load dữ liệu (giả sử bạn đã có sẵn df)\ndf = pd.read_csv(\"/kaggle/input/data-full-features-ai/weather_data_nghean (1).csv\")\n\n# Kiểm tra thông tin tổng quát\nprint(df.info())\nprint(df.describe())\n\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:59:07.682124Z","iopub.execute_input":"2025-05-05T08:59:07.682306Z","iopub.status.idle":"2025-05-05T08:59:15.683104Z","shell.execute_reply.started":"2025-05-05T08:59:07.682290Z","shell.execute_reply":"2025-05-05T08:59:15.682199Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 388493 entries, 0 to 388492\nData columns (total 38 columns):\n #   Column    Non-Null Count   Dtype  \n---  ------    --------------   -----  \n 0   x         388493 non-null  float64\n 1   y         388493 non-null  float64\n 2   B04B      388493 non-null  float64\n 3   B05B      388493 non-null  float64\n 4   B06B      388493 non-null  float64\n 5   B09B      388493 non-null  float64\n 6   B10B      388493 non-null  float64\n 7   B11B      388493 non-null  float64\n 8   B12B      388493 non-null  float64\n 9   B14B      388493 non-null  float64\n 10  B16B      388493 non-null  float64\n 11  I2B       388493 non-null  float64\n 12  I4B       388493 non-null  float64\n 13  IRB       388493 non-null  float64\n 14  VSB       388493 non-null  float64\n 15  WVB       388493 non-null  float64\n 16  CAPE      388493 non-null  float64\n 17  CIN       388493 non-null  float64\n 18  EWSS      388493 non-null  float64\n 19  IE        388493 non-null  float64\n 20  ISOR      388493 non-null  float64\n 21  KX        388493 non-null  float64\n 22  PEV       388493 non-null  float64\n 23  R250      388493 non-null  float64\n 24  R500      388493 non-null  float64\n 25  R850      388493 non-null  float64\n 26  SLHF      388493 non-null  float64\n 27  SLOR      388493 non-null  float64\n 28  SSHF      388493 non-null  float64\n 29  TCLW      388493 non-null  float64\n 30  TCW       388493 non-null  float64\n 31  TCWV      388493 non-null  float64\n 32  U250      388493 non-null  float64\n 33  U850      388493 non-null  float64\n 34  V250      388493 non-null  float64\n 35  V850      388493 non-null  float64\n 36  Radar     388493 non-null  float64\n 37  datetime  388493 non-null  object \ndtypes: float64(37), object(1)\nmemory usage: 112.6+ MB\nNone\n                   x              y           B04B           B05B  \\\ncount  388493.000000  388493.000000  388493.000000  388493.000000   \nmean      104.949809      19.234241       0.294234       0.174581   \nstd         0.437629       0.310876       0.199693       0.123118   \nmin       103.900000      18.560000       0.000391       0.000326   \n25%       104.620000      19.000000       0.141541       0.080861   \n50%       104.980000      19.240000       0.251780       0.155233   \n75%       105.300000      19.440000       0.437042       0.253082   \nmax       105.780000      19.960000       0.923014       0.612529   \n\n                B06B           B09B           B10B           B11B  \\\ncount  388493.000000  388493.000000  388493.000000  388493.000000   \nmean        0.129021     243.419956     250.408793     271.419048   \nstd         0.095252      10.202720      12.674594      21.098207   \nmin         0.000326     198.584960     169.366670     169.545000   \n25%         0.041728     238.151980     244.618580     259.743530   \n50%         0.118703     245.584880     254.291060     278.781040   \n75%         0.209381     250.669970     259.575560     287.011350   \nmax         0.433777     262.196000     268.004850     303.228880   \n\n                B12B           B14B  ...           SLOR          SSHF  \\\ncount  388493.000000  388493.000000  ...  388493.000000  3.884930e+05   \nmean      255.579941     272.033474  ...       0.028054 -9.036349e+04   \nstd        13.180805      22.568102  ...       0.014501  1.854509e+05   \nmin       176.324800     169.685610  ...       0.000701 -1.469406e+06   \n25%       248.202160     258.477700  ...       0.016802 -1.607370e+05   \n50%       260.255700     280.006770  ...       0.028084 -2.627300e+04   \n75%       265.263920     289.189640  ...       0.040154  2.112800e+04   \nmax       275.978500     306.162450  ...       0.058033  3.990850e+05   \n\n                TCLW            TCW           TCWV           U250  \\\ncount  388493.000000  388493.000000  388493.000000  388493.000000   \nmean        0.219809      43.269879      42.926766      11.939170   \nstd         0.232675       8.946554       8.712348      15.162809   \nmin         0.000000      14.542634      14.542566     -22.836090   \n25%         0.059570      36.939102      36.731980      -3.439392   \n50%         0.128784      42.879913      42.662720      15.895691   \n75%         0.311035      49.382996      48.897170      24.974121   \nmax         1.454712      75.416110      71.073074      42.879456   \n\n                U850           V250           V850          Radar  \ncount  388493.000000  388493.000000  388493.000000  388493.000000  \nmean       -1.525150       4.590938      -0.703348       0.204880  \nstd         5.760290       7.601398       5.227427       1.360052  \nmin       -24.732285     -13.504059     -22.774704       0.000000  \n25%        -4.977386      -0.895676      -3.536209       0.000000  \n50%        -1.275269       3.602570       0.969681       0.000000  \n75%         2.725372       9.770599       3.042831       0.000000  \nmax        12.849945      27.846588       9.968582      82.570000  \n\n[8 rows x 37 columns]\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"        x      y      B04B      B05B      B06B       B09B       B10B  \\\n0  104.90  19.96  0.498362  0.352224  0.236776  255.42627  260.79110   \n1  104.94  19.96  0.498362  0.352224  0.236776  255.42627  260.79110   \n2  104.98  19.96  0.572723  0.384196  0.249166  255.30000  260.90370   \n3  104.86  19.92  0.532949  0.360718  0.238078  255.81377  260.79684   \n4  104.90  19.92  0.532949  0.360718  0.238078  255.81377  260.79684   \n\n        B11B      B12B       B14B  ...      SSHF      TCLW        TCW  \\\n0  279.25586  259.7476  281.53525  ... -137404.0  0.601746  35.615920   \n1  279.25586  259.7476  281.53525  ... -137404.0  0.601746  35.615920   \n2  280.62646  260.5460  283.24900  ... -137404.0  0.601746  35.615920   \n3  278.82367  259.3540  280.84116  ... -272124.0  0.550171  32.744827   \n4  278.82367  259.3540  280.84116  ... -137404.0  0.601746  35.615920   \n\n        TCWV       U250      U850      V250      V850  Radar  \\\n0  35.005510  25.895142 -4.906418  6.482254  5.172928    0.0   \n1  35.005510  25.895142 -4.906418  6.482254  5.172928    0.0   \n2  35.005510  25.895142 -4.906418  6.482254  5.172928    0.0   \n3  32.179337  26.195923 -4.334152  6.599442  3.686600    0.0   \n4  35.005510  25.895142 -4.906418  6.482254  5.172928    0.0   \n\n              datetime  \n0  2019-04-01 08:00:00  \n1  2019-04-01 08:00:00  \n2  2019-04-01 08:00:00  \n3  2019-04-01 08:00:00  \n4  2019-04-01 08:00:00  \n\n[5 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>B04B</th>\n      <th>B05B</th>\n      <th>B06B</th>\n      <th>B09B</th>\n      <th>B10B</th>\n      <th>B11B</th>\n      <th>B12B</th>\n      <th>B14B</th>\n      <th>...</th>\n      <th>SSHF</th>\n      <th>TCLW</th>\n      <th>TCW</th>\n      <th>TCWV</th>\n      <th>U250</th>\n      <th>U850</th>\n      <th>V250</th>\n      <th>V850</th>\n      <th>Radar</th>\n      <th>datetime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>104.90</td>\n      <td>19.96</td>\n      <td>0.498362</td>\n      <td>0.352224</td>\n      <td>0.236776</td>\n      <td>255.42627</td>\n      <td>260.79110</td>\n      <td>279.25586</td>\n      <td>259.7476</td>\n      <td>281.53525</td>\n      <td>...</td>\n      <td>-137404.0</td>\n      <td>0.601746</td>\n      <td>35.615920</td>\n      <td>35.005510</td>\n      <td>25.895142</td>\n      <td>-4.906418</td>\n      <td>6.482254</td>\n      <td>5.172928</td>\n      <td>0.0</td>\n      <td>2019-04-01 08:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>104.94</td>\n      <td>19.96</td>\n      <td>0.498362</td>\n      <td>0.352224</td>\n      <td>0.236776</td>\n      <td>255.42627</td>\n      <td>260.79110</td>\n      <td>279.25586</td>\n      <td>259.7476</td>\n      <td>281.53525</td>\n      <td>...</td>\n      <td>-137404.0</td>\n      <td>0.601746</td>\n      <td>35.615920</td>\n      <td>35.005510</td>\n      <td>25.895142</td>\n      <td>-4.906418</td>\n      <td>6.482254</td>\n      <td>5.172928</td>\n      <td>0.0</td>\n      <td>2019-04-01 08:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104.98</td>\n      <td>19.96</td>\n      <td>0.572723</td>\n      <td>0.384196</td>\n      <td>0.249166</td>\n      <td>255.30000</td>\n      <td>260.90370</td>\n      <td>280.62646</td>\n      <td>260.5460</td>\n      <td>283.24900</td>\n      <td>...</td>\n      <td>-137404.0</td>\n      <td>0.601746</td>\n      <td>35.615920</td>\n      <td>35.005510</td>\n      <td>25.895142</td>\n      <td>-4.906418</td>\n      <td>6.482254</td>\n      <td>5.172928</td>\n      <td>0.0</td>\n      <td>2019-04-01 08:00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>104.86</td>\n      <td>19.92</td>\n      <td>0.532949</td>\n      <td>0.360718</td>\n      <td>0.238078</td>\n      <td>255.81377</td>\n      <td>260.79684</td>\n      <td>278.82367</td>\n      <td>259.3540</td>\n      <td>280.84116</td>\n      <td>...</td>\n      <td>-272124.0</td>\n      <td>0.550171</td>\n      <td>32.744827</td>\n      <td>32.179337</td>\n      <td>26.195923</td>\n      <td>-4.334152</td>\n      <td>6.599442</td>\n      <td>3.686600</td>\n      <td>0.0</td>\n      <td>2019-04-01 08:00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>104.90</td>\n      <td>19.92</td>\n      <td>0.532949</td>\n      <td>0.360718</td>\n      <td>0.238078</td>\n      <td>255.81377</td>\n      <td>260.79684</td>\n      <td>278.82367</td>\n      <td>259.3540</td>\n      <td>280.84116</td>\n      <td>...</td>\n      <td>-137404.0</td>\n      <td>0.601746</td>\n      <td>35.615920</td>\n      <td>35.005510</td>\n      <td>25.895142</td>\n      <td>-4.906418</td>\n      <td>6.482254</td>\n      <td>5.172928</td>\n      <td>0.0</td>\n      <td>2019-04-01 08:00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 38 columns</p>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"HIMA_BANDS = ['B04B', 'B05B', 'B06B', 'B09B', 'B10B', 'B11B', 'B12B', 'B14B', 'B16B', 'I2B', 'I4B', 'IRB', 'VSB', 'WVB']\nERA5_PARAMS = ['CAPE', 'CIN', 'EWSS', 'IE', 'ISOR', 'KX', 'PEV', 'R250', 'R500', 'R850', 'SLHF', 'SLOR', 'SSHF', 'TCLW', 'TCW', 'TCWV', 'U250', 'U850', 'V250', 'V850']\nSELECTED_HIMA_BANDS = [ 'B05B', 'B06B',  'B10B', 'B11B', 'B12B',  'I4B', 'IRB']\nSELECTED_ERA5_PARAMS = ['CAPE', 'CIN', 'EWSS', 'IE', 'ISOR', 'KX', 'PEV', 'R250', 'R500', 'R850', 'SLHF', 'SLOR', 'SSHF', 'TCLW', 'TCW', 'TCWV', 'U250', 'U850', 'V250', 'V850']\nSELECTED_FEATURES = SELECTED_HIMA_BANDS + SELECTED_ERA5_PARAMS\nHEIGHT, WIDTH = 90, 250","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:59:15.684014Z","iopub.execute_input":"2025-05-05T08:59:15.684357Z","iopub.status.idle":"2025-05-05T08:59:15.690244Z","shell.execute_reply.started":"2025-05-05T08:59:15.684337Z","shell.execute_reply":"2025-05-05T08:59:15.689356Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom scipy.stats import pearsonr\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom cuml.ensemble import RandomForestRegressor\n\n#========== Các hàm tiện ích chung ==========\ndef filter_features(X, selected_features, feature_names=None):\n    if isinstance(X, pd.DataFrame):\n        missing = set(selected_features) - set(X.columns)\n        if missing:\n            raise ValueError(f\"Missing features: {missing}\")\n        return X[selected_features]\n    elif isinstance(X, np.ndarray):\n        if feature_names is None:\n            raise ValueError(\"Feature names chưa được định nghĩa cho numpy array\")\n        X_df = pd.DataFrame(X, columns=feature_names)\n        return X_df[selected_features].values\n    else:\n        raise TypeError(\"Đầu vào phải là DataFrame hoặc numpy array\")\n\n#========== Hàm cho XGBoost ==========\ndef train_xgb(X, y, selected_features, params=None, sample_weight=None, n_splits=5):\n    default_params = {\n         'n_estimators': 500\n    }\n    final_params = {**default_params, **(params or {})}\n    \n    feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else None\n    X_filtered = filter_features(X, selected_features, feature_names)\n    \n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    train_idx, val_idx = next(kf.split(X_filtered))\n    \n    # Trích xuất trọng số cho tập train\n    if sample_weight is not None:\n        train_weight = sample_weight[train_idx]\n        val_weight = sample_weight[val_idx]\n    else:\n        train_weight = val_weight = None\n\n    X_train = X_filtered.iloc[train_idx] if isinstance(X, pd.DataFrame) else X_filtered[train_idx]\n    y_train = y.iloc[train_idx] if isinstance(y, pd.Series) else y[train_idx]\n    X_val = X_filtered.iloc[val_idx] if isinstance(X, pd.DataFrame) else X_filtered[val_idx]\n    y_val = y.iloc[val_idx] if isinstance(y, pd.Series) else y[val_idx]\n\n    model = xgb.XGBRegressor(**final_params)\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        sample_weight=train_weight,  # Thêm trọng số\n        early_stopping_rounds=10,\n        verbose=False\n    )\n    \n    return {\n        'model': model,\n        'feature_names': feature_names,\n        'selected_features': selected_features\n    }\n\n\ndef predict_xgb(model_dict, X):\n    X_filtered = filter_features(\n        X, \n        model_dict['selected_features'], \n        model_dict['feature_names']\n    )\n    return model_dict['model'].predict(X_filtered)\n\n#========== Hàm cho LightGBM ==========\ndef train_lgb(X, y, selected_features, params=None, n_splits=5):\n    default_params = {\n        'n_estimators': 500\n    }\n    final_params = {**default_params, **(params or {})}\n    \n    feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else None\n    X_filtered = filter_features(X, selected_features, feature_names)\n    \n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    train_idx, val_idx = next(kf.split(X_filtered))\n    \n    X_train = X_filtered.iloc[train_idx] if isinstance(X, pd.DataFrame) else X_filtered[train_idx]\n    y_train = y.iloc[train_idx] if isinstance(y, pd.Series) else y[train_idx]\n    X_val = X_filtered.iloc[val_idx] if isinstance(X, pd.DataFrame) else X_filtered[val_idx]\n    y_val = y.iloc[val_idx] if isinstance(y, pd.Series) else y[val_idx]\n\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n    \n    model = lgb.train(\n        final_params,\n        train_data,\n        valid_sets=[val_data],\n        num_boost_round=100,\n        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)]\n    )\n    \n    return {\n        'model': model,\n        'feature_names': feature_names,\n        'selected_features': selected_features\n    }\n\ndef predict_lgb(model_dict, X):\n    X_filtered = filter_features(\n        X,\n        model_dict['selected_features'],\n        model_dict['feature_names']\n    )\n    return model_dict['model'].predict(X_filtered)\n\n#========== Hàm cho Random Forest ==========\ndef train_ert(X, y, selected_features, params=None):\n    default_params = {'n_estimators': 500}\n    final_params = {**default_params, **(params or {})}\n    \n    feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else None\n    X_filtered = filter_features(X, selected_features, feature_names)\n    \n    model = RandomForestRegressor(**final_params)\n    model.fit(X_filtered, y)\n    \n    return {\n        'model': model,\n        'feature_names': feature_names,\n        'selected_features': selected_features\n    }\n\ndef predict_ert(model_dict, X):\n    X_filtered = filter_features(\n        X,\n        model_dict['selected_features'],\n        model_dict['feature_names']\n    )\n    return model_dict['model'].predict(X_filtered)\n\n#========== Hàm cho Stacking ==========\ndef generate_meta_features(X, y, base_models, n_folds=5):\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    meta = np.zeros((X.shape[0], len(base_models)))\n    \n    for i, model_dict in enumerate(base_models):\n        for train_idx, val_idx in kf.split(X):\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train = y[train_idx]\n            \n            if 'xgb' in model_dict['type']:\n                m = train_xgb(\n                    pd.DataFrame(X_train, columns=model_dict['feature_names']),\n                    y_train,\n                    model_dict['selected_features'],\n                    params=model_dict.get('params', None)\n                )\n                preds = predict_xgb(m, pd.DataFrame(X_val, columns=model_dict['feature_names']))\n            elif 'lgb' in model_dict['type']:\n                m = train_lgb(\n                    pd.DataFrame(X_train, columns=model_dict['feature_names']),\n                    y_train,\n                    model_dict['selected_features'],\n                    params=model_dict.get('params', None)\n                )\n                preds = predict_lgb(m, pd.DataFrame(X_val, columns=model_dict['feature_names']))\n            elif 'ert' in model_dict['type']:\n                m = train_ert(\n                    pd.DataFrame(X_train, columns=model_dict['feature_names']),\n                    y_train,\n                    model_dict['selected_features'],\n                    params=model_dict.get('params', None)\n                )\n                preds = predict_ert(m, pd.DataFrame(X_val, columns=model_dict['feature_names']))\n            \n            meta[val_idx, i] = preds\n            \n    return meta\n\ndef train_stacking(X, y, base_models, level2_model, n_folds=5):\n    meta_features = generate_meta_features(X.values, y.values, base_models, n_folds)\n    level2_model.fit(meta_features, y.values)\n    return level2_model, [model['feature_names'] for model in base_models]\n\ndef predict_stacking(stacking_model, base_models_info, X):\n    meta_test = []\n    for model_info in base_models_info:\n        if model_info['type'] == 'xgb':\n            preds = predict_xgb(model_info, X)\n        elif model_info['type'] == 'lgb':\n            preds = predict_lgb(model_info, X)\n        elif model_info['type'] == 'ert':\n            preds = predict_ert(model_info, X)\n        meta_test.append(preds)\n    return stacking_model.predict(np.column_stack(meta_test))\n\n#========== Pipeline chính ==========\n# Load data và chuẩn bị features\ndf = pd.read_csv(\"/kaggle/input/data-full-features-ai/weather_data_nghean (1).csv\")\ndf.fillna(df.mean(numeric_only=True), inplace=True)\n\nTARGET_COL = \"Radar\"\nHIMA_BANDS = [ 'B09B', 'B10B', 'B11B', 'B12B', 'B14B', 'B16B', 'I2B', 'I4B', 'IRB', 'VSB', 'WVB']\nERA5_PARAMS = [ 'R500', 'TCLW', 'TCW', 'TCWV']\nFOR_XG = HIMA_BANDS + ERA5_PARAMS\nALL_FEATURES = FOR_XG\n\nX = df[ALL_FEATURES]\ny = df[TARGET_COL]\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Huấn luyện các model level-1\ntrain_weight = np.where(y_train > 0, 5.0, 1.0)\nxgb_model = train_xgb(\n    X_train, y_train,\n    selected_features=FOR_XG,\n    params={},\n    sample_weight=train_weight\n)\n\nlgb_model = train_lgb(\n    X_train, y_train,\n    selected_features=ALL_FEATURES,\n    params={}\n)\n\nert_model = train_ert(\n    X_train, y_train,\n    selected_features=ALL_FEATURES\n)\n\n# Đánh giá các model\ndef evaluate_model(predict_func, model_dict, X_test, y_test):\n    preds = predict_func(model_dict, X_test)\n    cc, _ = pearsonr(y_test, preds)  # Tính Pearson Correlation Coefficient\n    return {\n        'MAE': mean_absolute_error(y_test, preds),\n        'RMSE': np.sqrt(mean_squared_error(y_test, preds)),\n        'R2': r2_score(y_test, preds),\n        'CC': cc\n    }\n\nmetrics = {\n    'XGB': evaluate_model(predict_xgb, xgb_model, X_test, y_test),\n    'LGB': evaluate_model(predict_lgb, lgb_model, X_test, y_test),\n    'ERT': evaluate_model(predict_ert, ert_model, X_test, y_test)\n}\n\n# Huấn luyện stacking model\nbase_models_info = [\n    {'type': 'xgb', **xgb_model},\n    {'type': 'lgb', **lgb_model},\n    {'type': 'ert', **ert_model}\n]\n\nlevel2_model = ElasticNet(alpha=0.01, l1_ratio=0.7)\nstacking_model, feature_names_list = train_stacking(\n    X_train, y_train,\n    base_models=base_models_info,\n    level2_model=level2_model\n)\n\n# Dự đoán và đánh giá stacking\ntest_preds = predict_stacking(\n    stacking_model,\n    base_models_info,\n    X_test\n)\n\nstacking_metrics = {\n    'MAE': mean_absolute_error(y_test, test_preds),\n    'RMSE': np.sqrt(mean_squared_error(y_test, test_preds)),\n    'R2': r2_score(y_test, test_preds),\n    'CC': pearsonr(y_test, test_preds)[0]  # Tính CC cho stacking\n}\n\nprint(\"Level-1 Metrics:\", metrics)\nprint(\"\\nStacking Metrics:\", stacking_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T08:59:15.692032Z","iopub.execute_input":"2025-05-05T08:59:15.692560Z","iopub.status.idle":"2025-05-05T09:01:16.537878Z","shell.execute_reply.started":"2025-05-05T08:59:15.692532Z","shell.execute_reply":"2025-05-05T09:01:16.537017Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028611 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3825\n[LightGBM] [Info] Number of data points in the train set: 248635, number of used features: 15\n[LightGBM] [Info] Start training from score 0.206359\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015067 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3825\n[LightGBM] [Info] Number of data points in the train set: 198908, number of used features: 15\n[LightGBM] [Info] Start training from score 0.206243\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034774 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3825\n[LightGBM] [Info] Number of data points in the train set: 198908, number of used features: 15\n[LightGBM] [Info] Start training from score 0.208835\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015932 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3825\n[LightGBM] [Info] Number of data points in the train set: 198908, number of used features: 15\n[LightGBM] [Info] Start training from score 0.205769\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3825\n[LightGBM] [Info] Number of data points in the train set: 198908, number of used features: 15\n[LightGBM] [Info] Start training from score 0.207625\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3825\n[LightGBM] [Info] Number of data points in the train set: 198908, number of used features: 15\n[LightGBM] [Info] Start training from score 0.207136\nLevel-1 Metrics: {'XGB': {'MAE': 0.21531215211032728, 'RMSE': 0.7184877964773241, 'R2': 0.7047441031822492, 'CC': 0.8401753992781928}, 'LGB': {'MAE': 0.15495848283011496, 'RMSE': 0.7146488731002453, 'R2': 0.7078908140518194, 'CC': 0.8434516246039292}, 'ERT': {'MAE': 0.1524852323909223, 'RMSE': 0.7468036534156097, 'R2': 0.681013241232385, 'CC': 0.8368500009413289}}\n\nStacking Metrics: {'MAE': 0.1671685710330947, 'RMSE': 0.6796675894062273, 'R2': 0.7357877744294636, 'CC': 0.8582933829560926}\n","output_type":"stream"}],"execution_count":3}]}