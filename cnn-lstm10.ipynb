{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-01T04:19:48.715749Z",
     "iopub.status.busy": "2025-05-01T04:19:48.715338Z",
     "iopub.status.idle": "2025-05-01T04:19:53.468818Z",
     "shell.execute_reply": "2025-05-01T04:19:53.467829Z",
     "shell.execute_reply.started": "2025-05-01T04:19:48.715718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GDAL in /usr/local/lib/python3.11/dist-packages (3.6.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install GDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:19:53.471148Z",
     "iopub.status.busy": "2025-05-01T04:19:53.470832Z",
     "iopub.status.idle": "2025-05-01T04:20:02.334885Z",
     "shell.execute_reply": "2025-05-01T04:20:02.333931Z",
     "shell.execute_reply.started": "2025-05-01T04:19:53.471112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import glob\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.336157Z",
     "iopub.status.busy": "2025-05-01T04:20:02.335775Z",
     "iopub.status.idle": "2025-05-01T04:20:02.345115Z",
     "shell.execute_reply": "2025-05-01T04:20:02.344077Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.336136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.347589Z",
     "iopub.status.busy": "2025-05-01T04:20:02.347279Z",
     "iopub.status.idle": "2025-05-01T04:20:02.380438Z",
     "shell.execute_reply": "2025-05-01T04:20:02.379462Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.347540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Đặt seed để tái lập kết quả\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Định nghĩa các hằng số và đường dẫn\n",
    "BASE_PATH = \"/kaggle/input/btl-ai/DATA_SV\"\n",
    "HIMA_PATH = os.path.join(BASE_PATH, \"Hima\")\n",
    "ERA5_PATH = os.path.join(BASE_PATH, \"ERA5\")\n",
    "PRECIP_PATH = os.path.join(BASE_PATH, \"Precipitation/Radar\")\n",
    "OUTPUT_PATH = \"/kaggle/working/output/\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.381774Z",
     "iopub.status.busy": "2025-05-01T04:20:02.381410Z",
     "iopub.status.idle": "2025-05-01T04:20:02.389879Z",
     "shell.execute_reply": "2025-05-01T04:20:02.388803Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.381750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "HIMA_BANDS = ['B04B', 'B05B', 'B06B', 'B09B', 'B10B', 'B11B', 'B12B', 'B14B', 'B16B', 'I2B', 'I4B', 'IRB', 'VSB', 'WVB']  # 14 band\n",
    "ERA5_PARAMS = ['CAPE', 'CIN', 'EWSS', 'IE', 'ISOR', 'KX', 'PEV', 'R250', 'R500', 'R850', 'SLHF', 'SLOR', 'SSHF', 'TCLW', 'TCW', 'TCWV', 'U250', 'U850', 'V250', 'V850']  # 20 tham số\n",
    "HEIGHT, WIDTH = 90, 250\n",
    "IN_CHANNEL = len(HIMA_BANDS) + len(ERA5_PARAMS)\n",
    "\n",
    "DOWNSCALE_FACTOR = 2\n",
    "NEW_HEIGHT, NEW_WIDTH = HEIGHT // DOWNSCALE_FACTOR, WIDTH // DOWNSCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.391331Z",
     "iopub.status.busy": "2025-05-01T04:20:02.390970Z",
     "iopub.status.idle": "2025-05-01T04:20:02.411224Z",
     "shell.execute_reply": "2025-05-01T04:20:02.410220Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.391284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Improved function to read GeoTIFF files\n",
    "def read_geotiff(file_path, data_type=\"Radar\"):\n",
    "    try:\n",
    "        ds = gdal.Open(file_path)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        band = ds.GetRasterBand(1)\n",
    "        nodata = band.GetNoDataValue()\n",
    "        data = band.ReadAsArray().astype(np.float32)\n",
    "        ds = None\n",
    "        \n",
    "        if data.shape != (HEIGHT, WIDTH):\n",
    "            print(f\"Invalid shape {data.shape} for file {file_path}, expected ({HEIGHT}, {WIDTH})\")\n",
    "            return None\n",
    "        \n",
    "        # Handle missing values\n",
    "        if nodata is not None:\n",
    "            mask = data == nodata\n",
    "            if np.any(mask):\n",
    "                if data_type == \"Radar\":\n",
    "                    # For radar data, missing values should be 0 (no rain)\n",
    "                    data[mask] = 0\n",
    "                else:  # Hima and ERA5\n",
    "                    # For satellite and meteorological data, use mean value imputation\n",
    "                    valid_data = data[~mask]\n",
    "                    if len(valid_data) > 0:\n",
    "                        mean_value = np.mean(valid_data)\n",
    "                        data[mask] = mean_value\n",
    "                    else:\n",
    "                        data[mask] = 0\n",
    "        \n",
    "        # Additional outlier handling\n",
    "        if data_type != \"Radar\":  # For non-radar data\n",
    "            # Remove extreme outliers (values beyond 5 standard deviations)\n",
    "            valid_mask = ~(np.isinf(data) | np.isnan(data) | (data == nodata))\n",
    "            if np.sum(valid_mask) > 0:\n",
    "                valid_data = data[valid_mask]\n",
    "                mean_val = np.mean(valid_data)\n",
    "                std_val = np.std(valid_data)\n",
    "                lower_bound = mean_val - 5 * std_val\n",
    "                upper_bound = mean_val + 5 * std_val\n",
    "                data = np.clip(data, lower_bound, upper_bound)\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.412875Z",
     "iopub.status.busy": "2025-05-01T04:20:02.412450Z",
     "iopub.status.idle": "2025-05-01T04:20:02.434112Z",
     "shell.execute_reply": "2025-05-01T04:20:02.433266Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.412844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hàm phân tích thời gian từ tên file\n",
    "def parse_datetime_from_filename(filename, data_type):\n",
    "    try:\n",
    "        if data_type == \"Hima\":\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 2:\n",
    "                return None\n",
    "            time_part = parts[1].split('_TB.tif')[0]\n",
    "            time_part = time_part.replace('.Z', '')\n",
    "            dt = datetime.strptime(time_part, '%Y%m%d%H%M')\n",
    "        elif data_type == \"ERA5\":\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 2:\n",
    "                return None\n",
    "            time_part = parts[1].replace('.tif', '')\n",
    "            dt = datetime.strptime(time_part, '%Y%m%d%H%M%S')\n",
    "        elif data_type == \"Radar\":\n",
    "            time_part = filename.split('_')[1].replace('.tif', '')\n",
    "            dt = datetime.strptime(time_part, '%Y%m%d%H%M%S')\n",
    "        else:\n",
    "            return None\n",
    "        return dt.replace(minute=0, second=0, microsecond=0)\n",
    "    except Exception as e:\n",
    "        global error_count\n",
    "        if error_count < 5:\n",
    "            print(f\"Error parsing datetime from {filename} (type {data_type}): {e}\")\n",
    "            error_count += 1\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.435777Z",
     "iopub.status.busy": "2025-05-01T04:20:02.435426Z",
     "iopub.status.idle": "2025-05-01T04:20:02.456469Z",
     "shell.execute_reply": "2025-05-01T04:20:02.455127Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.435738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "error_count = 0\n",
    "\n",
    "# Hàm thu thập file\n",
    "def collect_files(base_path, expected_subdirs=None, data_type=None):\n",
    "    files_dict = {}\n",
    "    file_count = 0\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.tif'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                dt = parse_datetime_from_filename(file, data_type)\n",
    "                if dt is None:\n",
    "                    continue\n",
    "                file_count += 1\n",
    "                if expected_subdirs:\n",
    "                    subdir = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(file_path)))))\n",
    "                    if dt not in files_dict:\n",
    "                        files_dict[dt] = {}\n",
    "                    files_dict[dt][subdir] = file_path\n",
    "                else:\n",
    "                    files_dict[dt] = file_path\n",
    "    print(f\"Found {file_count} files in {base_path}\")\n",
    "    return files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.458125Z",
     "iopub.status.busy": "2025-05-01T04:20:02.457794Z",
     "iopub.status.idle": "2025-05-01T04:20:02.478213Z",
     "shell.execute_reply": "2025-05-01T04:20:02.477127Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.458098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Improved data preprocessing function with adaptive normalization\n",
    "def preprocess_data(data, data_type):\n",
    "    if data is None:\n",
    "        return None\n",
    "    \n",
    "    data = np.where(np.isinf(data) | np.isnan(data) | (data < -9000), 0, data)\n",
    "    \n",
    "    if data_type == \"Radar\":\n",
    "        data = np.maximum(data, 0)\n",
    "        # Chỉ áp dụng log transform, không chia theo mức độ mưa\n",
    "        data_transformed = np.log1p(data)\n",
    "        # Chuẩn hóa về [0, 1] nhưng giữ độ tương phản\n",
    "        data_max = np.percentile(data_transformed, 99) if np.max(data_transformed) > 0 else 1.0\n",
    "        data_min = np.min(data_transformed)\n",
    "        range_val = data_max - data_min\n",
    "        if range_val > 0:\n",
    "            data_transformed = (data_transformed - data_min) / range_val\n",
    "        else:\n",
    "            data_transformed = np.zeros_like(data_transformed)\n",
    "        return data_transformed\n",
    "    else:\n",
    "        valid_data = data[~np.isnan(data) & ~np.isinf(data) & (data != -9999)]\n",
    "        if len(valid_data) == 0:\n",
    "            return np.zeros_like(data)\n",
    "        min_val = np.percentile(valid_data, 1)\n",
    "        max_val = np.percentile(valid_data, 99)\n",
    "        data = np.clip(data, min_val, max_val)\n",
    "        range_val = max_val - min_val\n",
    "        if range_val > 0:\n",
    "            data = (data - min_val) / range_val\n",
    "        else:\n",
    "            data = np.zeros_like(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.481468Z",
     "iopub.status.busy": "2025-05-01T04:20:02.481193Z",
     "iopub.status.idle": "2025-05-01T04:20:02.503739Z",
     "shell.execute_reply": "2025-05-01T04:20:02.502479Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.481447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FocalLoss nhị phân\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.clamp(y_pred, self.eps, 1 - self.eps)\n",
    "        bce = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        pt = torch.exp(-bce)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.505136Z",
     "iopub.status.busy": "2025-05-01T04:20:02.504783Z",
     "iopub.status.idle": "2025-05-01T04:20:02.525202Z",
     "shell.execute_reply": "2025-05-01T04:20:02.524129Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.505076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Sequence creation with adjusted balancing\n",
    "def create_time_sequences(hima_files, era5_files, precip_files, common_datetimes):\n",
    "    X, y = [], []\n",
    "    rain_count = 0\n",
    "    no_rain_count = 0\n",
    "    \n",
    "    for i in range(4, len(common_datetimes)):\n",
    "        dt = common_datetimes[i]\n",
    "        valid_sequence = True\n",
    "        for j in range(1, 5):\n",
    "            if common_datetimes[i-j] != dt - timedelta(hours=j):\n",
    "                valid_sequence = False\n",
    "                break\n",
    "        if not valid_sequence:\n",
    "            continue\n",
    "\n",
    "        sequence = []\n",
    "        sequence_valid = True\n",
    "        \n",
    "        for j in range(5):\n",
    "            dt_j = common_datetimes[i-4+j]\n",
    "            hima_data = []\n",
    "            for band in HIMA_BANDS:\n",
    "                file_path = hima_files.get(dt_j, {}).get(band)\n",
    "                if not file_path:\n",
    "                    sequence_valid = False\n",
    "                    break\n",
    "                data = read_geotiff(file_path, data_type=\"Hima\")\n",
    "                data = preprocess_data(data, \"Hima\")\n",
    "                if data is None:\n",
    "                    sequence_valid = False\n",
    "                    break\n",
    "                hima_data.append(data)\n",
    "            if not sequence_valid:\n",
    "                break\n",
    "            hima_data = np.stack(hima_data, axis=-1)\n",
    "\n",
    "            era5_data = []\n",
    "            for param in ERA5_PARAMS:\n",
    "                file_path = era5_files.get(dt_j, {}).get(param)\n",
    "                if not file_path:\n",
    "                    sequence_valid = False\n",
    "                    break\n",
    "                data = read_geotiff(file_path, data_type=\"ERA5\")\n",
    "                data = preprocess_data(data, \"ERA5\")\n",
    "                if data is None:\n",
    "                    sequence_valid = False\n",
    "                    break\n",
    "                era5_data.append(data)\n",
    "            if not sequence_valid:\n",
    "                break\n",
    "            era5_data = np.stack(era5_data, axis=-1)\n",
    "\n",
    "            combined = np.concatenate([hima_data, era5_data], axis=-1)\n",
    "            sequence.append(combined)\n",
    "        \n",
    "        if not sequence_valid:\n",
    "            continue\n",
    "\n",
    "        radar_file = precip_files.get(dt)\n",
    "        if not radar_file:\n",
    "            continue\n",
    "        radar_data = read_geotiff(radar_file, data_type=\"Radar\")\n",
    "        radar_data = preprocess_data(radar_data, \"Radar\")\n",
    "        if radar_data is None:\n",
    "            continue\n",
    "\n",
    "        has_rain = np.any(radar_data > 0.1)\n",
    "        if has_rain:\n",
    "            rain_count += 1\n",
    "        else:\n",
    "            no_rain_count += 1\n",
    "            if no_rain_count > rain_count * 2 and np.random.random() < 0.9:\n",
    "                continue\n",
    "\n",
    "        sequence_tensor = torch.tensor(np.array(sequence), dtype=torch.float32)\n",
    "        radar_tensor = torch.tensor(radar_data, dtype=torch.float32)\n",
    "        sequence_tensor = sequence_tensor.permute(0, 3, 1, 2)\n",
    "        radar_tensor = radar_tensor.unsqueeze(0).unsqueeze(0)\n",
    "        sequence_downsampled = F.avg_pool2d(sequence_tensor, kernel_size=DOWNSCALE_FACTOR, stride=DOWNSCALE_FACTOR)\n",
    "        radar_downsampled = F.max_pool2d(radar_tensor, kernel_size=DOWNSCALE_FACTOR, stride=DOWNSCALE_FACTOR)\n",
    "        sequence_downsampled = sequence_downsampled.permute(0, 2, 3, 1).numpy()\n",
    "        radar_downsampled = radar_downsampled.squeeze().numpy()\n",
    "\n",
    "        X.append(sequence_downsampled)\n",
    "        y.append(radar_downsampled)\n",
    "\n",
    "    print(f\"Created {len(X)} sequences ({rain_count} with rain, {no_rain_count} without rain)\")\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No valid sequences created. Check your data paths and filters.\")\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.transpose(0, 1, 4, 2, 3)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.526522Z",
     "iopub.status.busy": "2025-05-01T04:20:02.526179Z",
     "iopub.status.idle": "2025-05-01T04:20:02.548903Z",
     "shell.execute_reply": "2025-05-01T04:20:02.547791Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.526500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Định nghĩa lớp ConvLSTMCell tùy chỉnh\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels + out_channels, 4 * out_channels, kernel_size,\n",
    "            padding=padding, bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # x: (batch, in_channels, height, width)\n",
    "        # h_prev, c_prev: (batch, out_channels, height, width)\n",
    "        combined = torch.cat([x, h_prev], dim=1)  # (batch, in_channels + out_channels, height, width)\n",
    "        conv_out = self.conv(combined)  # (batch, 4 * out_channels, height, width)\n",
    "        i, f, o, g = torch.chunk(conv_out, 4, dim=1)  # Mỗi cái: (batch, out_channels, height, width)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.550422Z",
     "iopub.status.busy": "2025-05-01T04:20:02.550095Z",
     "iopub.status.idle": "2025-05-01T04:20:02.571616Z",
     "shell.execute_reply": "2025-05-01T04:20:02.570772Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.550392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ConvLSTM2d\n",
    "class ConvLSTM2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super(ConvLSTM2d, self).__init__()\n",
    "        self.cell = ConvLSTMCell(in_channels, out_channels, kernel_size, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, channels, height, width = x.size()\n",
    "        h = torch.zeros(batch, self.cell.out_channels, height, width, device=x.device)\n",
    "        c = torch.zeros(batch, self.cell.out_channels, height, width, device=x.device)\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.cell(x[:, t], h, c)\n",
    "        return h, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.573740Z",
     "iopub.status.busy": "2025-05-01T04:20:02.572818Z",
     "iopub.status.idle": "2025-05-01T04:20:02.591449Z",
     "shell.execute_reply": "2025-05-01T04:20:02.590520Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.573702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Attention module for focusing on important features\n",
    "# class SpatialAttention(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super(SpatialAttention, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=1, bias=False)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # Generate attention map\n",
    "#         attn = self.conv1(x)\n",
    "#         attn = torch.sigmoid(attn)\n",
    "        \n",
    "#         # Apply attention\n",
    "#         return x * attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.592897Z",
     "iopub.status.busy": "2025-05-01T04:20:02.592540Z",
     "iopub.status.idle": "2025-05-01T04:20:02.620517Z",
     "shell.execute_reply": "2025-05-01T04:20:02.619615Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.592869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=100, patience=15):\n",
    "    criterion = FocalLoss(gamma=2.0, alpha=0.25, beta=0.95)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), os.path.join(OUTPUT_PATH, 'best_model.pth'))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'training_curve.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.622048Z",
     "iopub.status.busy": "2025-05-01T04:20:02.621450Z",
     "iopub.status.idle": "2025-05-01T04:20:02.642586Z",
     "shell.execute_reply": "2025-05-01T04:20:02.641646Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.622024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mô hình đơn giản hóa\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.convlstm1 = ConvLSTM2d(in_channels, 64, kernel_size=(5, 5), padding=(2, 2))\n",
    "        self.convlstm2 = ConvLSTM2d(64, 32, kernel_size=(3, 3), padding=(1, 1))\n",
    "        self.conv_out = nn.Conv2d(32, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.convlstm1(x)\n",
    "        x, _ = self.convlstm2(x)\n",
    "        x = self.conv_out(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.643916Z",
     "iopub.status.busy": "2025-05-01T04:20:02.643633Z",
     "iopub.status.idle": "2025-05-01T04:20:02.660349Z",
     "shell.execute_reply": "2025-05-01T04:20:02.659631Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.643895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adjusted evaluation function with multiple thresholds\n",
    "def evaluate_model(y_true, y_pred, thresholds=[0.1, 0.2, 0.3]):\n",
    "    y_true_flat = y_true.reshape(-1)\n",
    "    y_pred_flat = y_pred.reshape(-1)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))\n",
    "    if np.std(y_true_flat) > 0 and np.std(y_pred_flat) > 0:\n",
    "        corr = np.corrcoef(y_true_flat, y_pred_flat)[0, 1]\n",
    "    else:\n",
    "        corr = 0\n",
    "    \n",
    "    results = {\n",
    "        'rmse': rmse,\n",
    "        'corr': corr\n",
    "    }\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_true_bin = (y_true_flat > 0.1).astype(int)\n",
    "        y_pred_bin = (y_pred_flat > threshold).astype(int)\n",
    "        hits = np.sum((y_true_bin == 1) & (y_pred_bin == 1))\n",
    "        misses = np.sum((y_true_bin == 1) & (y_pred_bin == 0))\n",
    "        false_alarms = np.sum((y_true_bin == 0) & (y_pred_bin == 1))\n",
    "        true_negatives = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n",
    "        total = hits + misses + false_alarms + true_negatives\n",
    "        accuracy = (hits + true_negatives) / total if total > 0 else 0\n",
    "        pod = hits / (hits + misses) if (hits + misses) > 0 else 0\n",
    "        far = false_alarms / (hits + false_alarms) if (hits + false_alarms) > 0 else 0\n",
    "        csi = hits / (hits + misses + false_alarms) if (hits + misses + false_alarms) > 0 else 0\n",
    "        denominator = ((hits + misses) * (misses + true_negatives) + \n",
    "                       (hits + false_alarms) * (false_alarms + true_negatives))\n",
    "        hss = (2 * (hits * true_negatives - misses * false_alarms)) / denominator if denominator > 0 else 0\n",
    "        random_hits = (hits + misses) * (hits + false_alarms) / total if total > 0 else 0\n",
    "        ets_denom = (hits + misses + false_alarms - random_hits)\n",
    "        ets = (hits - random_hits) / ets_denom if ets_denom > 0 else 0\n",
    "        \n",
    "        threshold_str = str(threshold).replace('.', '_')\n",
    "        results[f'accuracy_{threshold_str}'] = accuracy\n",
    "        results[f'pod_{threshold_str}'] = pod\n",
    "        results[f'far_{threshold_str}'] = far\n",
    "        results[f'csi_{threshold_str}'] = csi\n",
    "        results[f'hss_{threshold_str}'] = hss\n",
    "        results[f'ets_{threshold_str}'] = ets\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.661679Z",
     "iopub.status.busy": "2025-05-01T04:20:02.661320Z",
     "iopub.status.idle": "2025-05-01T04:20:02.683661Z",
     "shell.execute_reply": "2025-05-01T04:20:02.682638Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.661656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "def plot_scatter(y_true, y_pred, output_path):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    from scipy.stats import gaussian_kde\n",
    "    xy = np.vstack([y_true.flatten(), y_pred.flatten()])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    idx = z.argsort()\n",
    "    x, y, z = y_true.flatten()[idx], y_pred.flatten()[idx], z[idx]\n",
    "    plt.scatter(x, y, c=z, s=10, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('Ground Truth (Normalized Rainfall)')\n",
    "    plt.ylabel('Predicted (Normalized Rainfall)')\n",
    "    plt.title('Scatter Plot: Predicted vs Ground Truth')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_rainfall_map(y_true, y_pred, output_path):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax1.set_title('Ground Truth')\n",
    "    ax2.set_title('Prediction')\n",
    "    for ax, data in [(ax1, y_true), (ax2, y_pred)]:\n",
    "        ax.coastlines()\n",
    "        ax.add_feature(cfeature.BORDERS)\n",
    "        im = ax.imshow(data, cmap='Blues', origin='upper', transform=ccrs.PlateCarree(), vmin=0, vmax=np.max([y_true.max(), y_pred.max()]))\n",
    "        plt.colorbar(im, ax=ax, label='Rainfall (Normalized)')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def save_geotiff(data, output_path, reference_file):\n",
    "    ds = gdal.Open(reference_file)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_ds = driver.Create(output_path, NEW_WIDTH, NEW_HEIGHT, 1, gdal.GDT_Float32)\n",
    "    geo_transform = list(ds.GetGeoTransform())\n",
    "    geo_transform[1] *= DOWNSCALE_FACTOR\n",
    "    geo_transform[5] *= DOWNSCALE_FACTOR\n",
    "    out_ds.SetGeoTransform(tuple(geo_transform))\n",
    "    out_ds.SetProjection(ds.GetProjection())\n",
    "    out_band = out_ds.GetRasterBand(1)\n",
    "    out_band.WriteArray(data)\n",
    "    out_band.FlushCache()\n",
    "    out_ds = None\n",
    "    ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:02.685586Z",
     "iopub.status.busy": "2025-05-01T04:20:02.684750Z",
     "iopub.status.idle": "2025-05-01T04:20:34.029794Z",
     "shell.execute_reply": "2025-05-01T04:20:34.028928Z",
     "shell.execute_reply.started": "2025-05-01T04:20:02.685535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Himawari files...\n",
      "Found 1438 files in /kaggle/input/btl-ai/DATA_SV/Hima/B04B\n",
      "Found 1361 files in /kaggle/input/btl-ai/DATA_SV/Hima/B05B\n",
      "Found 1158 files in /kaggle/input/btl-ai/DATA_SV/Hima/B06B\n",
      "Found 2777 files in /kaggle/input/btl-ai/DATA_SV/Hima/B09B\n",
      "Found 2777 files in /kaggle/input/btl-ai/DATA_SV/Hima/B10B\n",
      "Found 2777 files in /kaggle/input/btl-ai/DATA_SV/Hima/B11B\n",
      "Found 2777 files in /kaggle/input/btl-ai/DATA_SV/Hima/B12B\n",
      "Found 2776 files in /kaggle/input/btl-ai/DATA_SV/Hima/B14B\n",
      "Found 2776 files in /kaggle/input/btl-ai/DATA_SV/Hima/B16B\n",
      "Found 2776 files in /kaggle/input/btl-ai/DATA_SV/Hima/I2B\n",
      "Found 2673 files in /kaggle/input/btl-ai/DATA_SV/Hima/I4B\n",
      "Found 2776 files in /kaggle/input/btl-ai/DATA_SV/Hima/IRB\n",
      "Found 1448 files in /kaggle/input/btl-ai/DATA_SV/Hima/VSB\n",
      "Found 2774 files in /kaggle/input/btl-ai/DATA_SV/Hima/WVB\n"
     ]
    }
   ],
   "source": [
    "# Bắt đầu chương trình\n",
    "print(\"Collecting Himawari files...\")\n",
    "hima_files = {}\n",
    "for band in HIMA_BANDS:\n",
    "    band_path = os.path.join(HIMA_PATH, band)\n",
    "    if not os.path.exists(band_path):\n",
    "        print(f\"Directory not found: {band_path}\")\n",
    "        continue\n",
    "    band_files = collect_files(band_path, expected_subdirs=HIMA_BANDS, data_type=\"Hima\")\n",
    "    for dt, paths in band_files.items():\n",
    "        if dt not in hima_files:\n",
    "            hima_files[dt] = {}\n",
    "        hima_files[dt][band] = paths[band]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:20:34.031055Z",
     "iopub.status.busy": "2025-05-01T04:20:34.030750Z",
     "iopub.status.idle": "2025-05-01T04:21:32.677691Z",
     "shell.execute_reply": "2025-05-01T04:21:32.676752Z",
     "shell.execute_reply.started": "2025-05-01T04:20:34.031030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ERA5 files...\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/CAPE\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/CIN\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/EWSS\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/IE\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/ISOR\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/KX\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/PEV\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/R250\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/R500\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/R850\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/SLHF\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/SLOR\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/SSHF\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/TCLW\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/TCW\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/TCWV\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/U250\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/U850\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/V250\n",
      "Found 2928 files in /kaggle/input/btl-ai/DATA_SV/ERA5/V850\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting ERA5 files...\")\n",
    "era5_files = {}\n",
    "for param in ERA5_PARAMS:\n",
    "    param_path = os.path.join(ERA5_PATH, param)\n",
    "    if not os.path.exists(param_path):\n",
    "        print(f\"Directory not found: {param_path}\")\n",
    "        continue\n",
    "    param_files = collect_files(param_path, expected_subdirs=ERA5_PARAMS, data_type=\"ERA5\")\n",
    "    for dt, paths in param_files.items():\n",
    "        if dt not in era5_files:\n",
    "            era5_files[dt] = {}\n",
    "        era5_files[dt][param] = paths[param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:21:32.678923Z",
     "iopub.status.busy": "2025-05-01T04:21:32.678638Z",
     "iopub.status.idle": "2025-05-01T04:21:34.896107Z",
     "shell.execute_reply": "2025-05-01T04:21:34.895284Z",
     "shell.execute_reply.started": "2025-05-01T04:21:32.678903Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Precipitation files...\n",
      "Found 2487 files in /kaggle/input/btl-ai/DATA_SV/Precipitation/Radar\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting Precipitation files...\")\n",
    "precip_files = collect_files(PRECIP_PATH, data_type=\"Radar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:21:34.897370Z",
     "iopub.status.busy": "2025-05-01T04:21:34.897051Z",
     "iopub.status.idle": "2025-05-01T04:21:34.904845Z",
     "shell.execute_reply": "2025-05-01T04:21:34.903758Z",
     "shell.execute_reply.started": "2025-05-01T04:21:34.897344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số thời điểm đồng bộ: 2337\n"
     ]
    }
   ],
   "source": [
    "# Đồng bộ thời gian\n",
    "common_datetimes = set(hima_files.keys()) & set(era5_files.keys()) & set(precip_files.keys())\n",
    "common_datetimes = sorted(list(common_datetimes))\n",
    "print(f\"Số thời điểm đồng bộ: {len(common_datetimes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T04:21:34.906508Z",
     "iopub.status.busy": "2025-05-01T04:21:34.905838Z",
     "iopub.status.idle": "2025-05-01T04:35:31.745136Z",
     "shell.execute_reply": "2025-05-01T04:35:31.743799Z",
     "shell.execute_reply.started": "2025-05-01T04:21:34.906477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu sau khi đồng bộ và resize: 2337\n",
      "Top 10 features có tương quan cao nhất với lượng mưa:\n",
      "B11B: 0.1550\n",
      "IRB: 0.1535\n",
      "I2B: 0.1512\n",
      "B14B: 0.1502\n",
      "I4B: 0.1451\n",
      "B16B: 0.1446\n",
      "B12B: 0.1425\n",
      "B10B: 0.1380\n",
      "TCW: 0.1326\n",
      "TCLW: 0.1290\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from skimage.transform import resize\n",
    "\n",
    "correlations = {}\n",
    "# Chỉ chọn những thời điểm có đủ cả Hima, ERA5, Radar\n",
    "common_datetimes = [dt for dt in common_datetimes if dt in precip_files]\n",
    "\n",
    "def resize_data(data, new_height, new_width):\n",
    "    return resize(data, (new_height, new_width), order=1, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "# Đọc và resize toàn bộ radar data trước\n",
    "radar_data_list = []\n",
    "for dt in common_datetimes:\n",
    "    radar = read_geotiff(precip_files[dt])\n",
    "    if radar is not None:\n",
    "        radar_resized = resize_data(radar, NEW_HEIGHT, NEW_WIDTH)\n",
    "        radar_data_list.append(radar_resized.flatten())\n",
    "    else:\n",
    "        # Nếu thiếu radar tại thời điểm nào thì bỏ luôn thời điểm đó\n",
    "        common_datetimes.remove(dt)\n",
    "\n",
    "radar_data_array = np.array(radar_data_list)  # shape = (num_samples, pixels)\n",
    "\n",
    "print(f\"Số mẫu sau khi đồng bộ và resize: {len(common_datetimes)}\")\n",
    "\n",
    "for band in HIMA_BANDS + ERA5_PARAMS:\n",
    "    feature_data_list = []\n",
    "    valid_datetimes = []\n",
    "    for dt in common_datetimes:\n",
    "        file_path = hima_files.get(dt, {}).get(band) or era5_files.get(dt, {}).get(band)\n",
    "        if file_path:\n",
    "            data = read_geotiff(file_path)\n",
    "            if data is not None:\n",
    "                resized_data = resize_data(data, NEW_HEIGHT, NEW_WIDTH)\n",
    "                feature_data_list.append(resized_data.flatten())\n",
    "                valid_datetimes.append(dt)\n",
    "    \n",
    "    # Chỉ giữ những radar sample ứng với những feature sample còn lại\n",
    "    if len(feature_data_list) >= 5:  # Chỉ tính nếu có ít nhất 5 sample (để Pearson meaningful)\n",
    "        feature_array = np.array(feature_data_list)\n",
    "        # Lọc radar data ứng với valid_datetimes\n",
    "        radar_matched = [radar_data_list[common_datetimes.index(dt)] for dt in valid_datetimes]\n",
    "        radar_array = np.array(radar_matched)\n",
    "\n",
    "        # Tính tương quan (flatten toàn bộ pixel)\n",
    "        corr, _ = pearsonr(feature_array.flatten(), radar_array.flatten())\n",
    "        correlations[band] = abs(corr)\n",
    "\n",
    "# Chọn top 10 features có tương quan cao nhất\n",
    "selected_features = sorted(correlations, key=correlations.get, reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 features có tương quan cao nhất với lượng mưa:\")\n",
    "for feat in selected_features:\n",
    "    print(f\"{feat}: {correlations[feat]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7020161,
     "sourceId": 11237152,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
